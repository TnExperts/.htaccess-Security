# Defining custom error document so that all restrictions are routed to a HTML page which minimizes the resource impact:
ErrorDocument 403 bad-bots.html

# Block bad bots & scrapers using their user-agents
#
# blocking content harvesters and site copiers using wget
SetEnvIfNoCase User-Agent .*wget.* bad_bot

# blocking content harvesters and site copiers using curl
SetEnvIfNoCase User-Agent .*curl.* bad_bot

# blocking content harvesters and site copiers using perl
SetEnvIfNoCase User-Agent .*libwww-perl.* bad_bot

# blocking vulnerability scanners
SetEnvIfNoCase User-Agent .*Acunetix.* bad_bot
SetEnvIfNoCase User-Agent .*FHscan.* bad_bot

# blocking aggressive chinese search engine
SetEnvIfNoCase User-Agent .*Baidu.* bad_bot
 
# blocking aggressive russian search engine
SetEnvIfNoCase User-Agent .*Yandex.* bad_bot

# blocking aggressive downloader scripts
SetEnvIfNoCase User-Agent .*Download\ Demon.* bad_bot
SetEnvIfNoCase User-Agent .*Download\ Devil.* bad_bot
SetEnvIfNoCase User-Agent .*Download\ Wonder.* bad_bot
SetEnvIfNoCase User-Agent .*EirGrabber.* bad_bot

# blocking email harvesters
SetEnvIfNoCase User-Agent .*EmailCollector.* bad_bot
SetEnvIfNoCase User-Agent .*EmailSiphon.* bad_bot
SetEnvIfNoCase User-Agent .*EmailWolf.* bad_bot

<Limit GET POST HEAD>
Order Allow,Deny
Allow from all
Deny from env=bad_bot
</Limit>
